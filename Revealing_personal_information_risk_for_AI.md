

When personal identifiable information (PII) or sensitive personal information (SPI) are used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage. This risk is amplified by amplified by generative AI

### Why is revealing personal information a concern for foundation models?

Sharing people's personal information impacts their rights and make them more vulnerable. Additionally, output data must be reviewed to comply with privacy laws and regulations. Business entities might face fines, reputational harms, disruption to operations, and other legal consequences if found in violation of data privacy or usage laws.

### Example - Exposure of personal information

Per the source article, ChatGPT suffered a bug and exposed titles and active users' chat history to other users. Later, OpenAI shared that even more private data from a small number of users was exposed including, active userâ€™s first and last name, email address, payment address, the last four digits of their credit card number, and credit card expiration date. In addition, it was reported that the payment-related information of 1.2% of ChatGPT Plus subscribers were also exposed in the outage.

Source: https://www.thehindubusinessline.com/info-tech/openai-admits-data-breach-at-chatgpt-private-data-of-premium-users-exposed/article66659944.ece
